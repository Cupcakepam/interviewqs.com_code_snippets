{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this to clean the html raw files that are downloaded from notebooks.  \n",
    "Flow is:  \n",
    "1 Run in ipynb, download raw html  \n",
    "2 Clean raw html using this ipynb, exports automatically  \n",
    "3 Dump everything in between guidelines (annoyingly can't import w/ JS due to search impact, etc)  \n",
    "3.5 Get rid of notebook container (2 first divs, as well as matching 2 divs in first chunk of code they're attached to), seems to be shifting the spacing, remove style tags (can do this in code eventually)  \n",
    "4 Update page title  \n",
    "5 Saving as htmlfilename_final -- this will align with the auto route generated from DDI trix  \n",
    "6 Ensure home page reflects addition  \n",
    "\n",
    "If images, need to save separately, load in manually.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath = '/Users/erikrood/Desktop/ddi_snippets/\\\n",
    "raw_ipynb_html_exports/pandas_rolling_mean.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the file\n",
    "with open(filepath, 'r') as file :\n",
    "  filedata = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#open file with beautiful soup, delete In/Out by calling class name\n",
    "with open(filepath, 'r') as file:\n",
    "    for line in file:\n",
    "        filedata = file.read()\n",
    "        soup = BeautifulSoup(filedata,\"html.parser\")\n",
    "        # replace with `soup.findAll` if you are using BeautifulSoup3\n",
    "        for div in soup.find_all(\"div\", {'class':'prompt output_prompt'}): \n",
    "            div.decompose()\n",
    "        for div in soup.find_all(\"div\", {'class':'prompt input_prompt'}): \n",
    "            div.decompose()\n",
    "        for i in range(0,len(soup.find_all(\"h4\"))-1): \n",
    "            newtg = soup.new_tag('br')\n",
    "            soup.find_all(\"h4\")[i+1].insert(0,newtg)\n",
    "        with open(filepath, 'wb') as file:\n",
    "                  file.write(soup.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#open BS4 processed file, remove anchor links + contents within using regex, write to final output\n",
    "with open(filepath, 'r') as file:\n",
    "    for line in file:\n",
    "        if '<body>' in line:                \n",
    "            for line in file: # now you are at the lines you want\n",
    "                filedata = file.read()\n",
    "                #strip anchor links\n",
    "                filedata = re.sub('<[aA][^>]*>([^<]+)</[aA]>', '', filedata)\n",
    "                \n",
    "                with open(filepath, 'w') as file:\n",
    "                  file.write(filedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
